###  目录

| list                                                         |
| ------------------------------------------------------------ |
| 2020_ECCV_Sep-Stereo Visually Guided Stereophonic Audio Generation by Associating Source Separation |
| 2021_ICCV_Localize_to_Binauralize_Audio_Spatialization_From_Visual_Sound_Source_Localization |
| 2021_ICME_WEAKLY-SUPERVISED AUDIO-VISUAL SOUND SOURCE DETECTION AND SEPARATION |
| 2021_ICCV_ACAV100M Automatic Curation of Large Scale Datasets for Audio-Visual Video Representation |

### MIDI

[如何看懂一份MIDI文件 ](http://www.jianshu.com/p/31d02765e1ec)?

MIDI(Musical Instrument Digital Interface)乐器数字接口 ，是20 世纪80 年代初为解决电声乐器之间的通信问题而提出的。MIDI是编曲界最广泛的音乐标准格式，可称为“计算机能理解的乐谱”。MIDI是电子乐器和计算机使用的标准语言，是一套消息（即指令）的约定，它不产生声音信号，而是在电缆传送各种消息，由接收消息的设备或其它电子装置产生声音或执行某个动作。

简单理解就是能从这一串编码中恢复出音乐

```
4D 54 68 64 00 00 00 06 00 01 00 03 01 E0 4D 54
72 6B 00 00 00 1A 00 FF 03 03 31 32 33 00 FF 51
03 08 7A 23 00 FF 58 04 04 02 18 08 00 FF 2F 00
4D 54 72 6B 00 00 00 67 00 FF 03 13 5B 47 4D 20
30 35 34 5D 20 56 6F 69 63 65 20 4F 6F 68 73 8F
00 90 3C 64 8C 18 80 3C 40 82 68 90 3E 64 8C 18
80 3E 40 82 68 90 40 64 86 48 80 40 40 78 90 41
64 86 48 80 41 40 78 90 43 64 86 48 80 43 40 78
90 45 64 86 48 80 45 40 78 90 47 64 87 40 80 47
40 87 40 90 48 64 8F 00 80 48 40 00 FF 2F 00 4D
54 72 6B 00 00 00 0E 00 FF 03 06 4D 61 72 6B 65
72 00 FF 2F 00
```

Midi数据流包含音轨数目，音高，力度，节拍速度，等等特征

### 音乐固有特征

#### 乐音（musical tone）

- 发音物体有规律地振动而产生的具有固定音高的音称乐音。如[钢琴](https://baike.baidu.com/item/钢琴)、[小提琴](https://baike.baidu.com/item/小提琴)、[二胡](https://baike.baidu.com/item/二胡)等都是能发出乐音的[乐器](https://baike.baidu.com/item/乐器)。乐音是音乐中所使用的最主要、最基本的材料，音乐中的[旋律](https://baike.baidu.com/item/旋律)、和声等均由乐音构成。

- 从声学的分析角度，乐音有三个主要特征:即[响度](https://baike.baidu.com/item/响度)（又称[音强](https://baike.baidu.com/item/音强)），[音调](https://baike.baidu.com/item/音调)（又称[音高](https://baike.baidu.com/item/音高)）和[音色](https://baike.baidu.com/item/音色)，称为乐音三要素。
- 在音乐使用的有固定音高的音（即乐音）的总和称[乐音体系](https://baike.baidu.com/item/乐音体系)。按现在通用的十二平均律，从最低音（每秒振动16次左右）到最高音（每秒振动4186次），整个乐音体系中约有97个音。乐音体系中的音，按照上行（xíng）即从低到高或下行（由高到低）的次序排列起来的音叫做[音列](https://baike.baidu.com/item/音列)。我们在钢琴上可以明显地看出乐音体系中所使用的音和音列。**现代标准的钢琴是音域最宽的乐器，有88个键，能奏出88个音高不同的乐音**,也就是说钢琴有88个音高各不相同的音。除此之外的音很少用在音乐中。

- 乐音体系中的各音叫做音级，分为基本音级和变化音级两种。 [2] 从最低音（最[长波](https://baike.baidu.com/item/长波/3252070)，[机械波](https://baike.baidu.com/item/机械波/2734777)27.5Hz左右，[波长](https://baike.baidu.com/item/波长/829184)最长，为12.5m）到最高音（最[短波](https://baike.baidu.com/item/短波/500185)，[机械波](https://baike.baidu.com/item/机械波/2734777)4186Hz左右，[波长](https://baike.baidu.com/item/波长/829184)最短，为8.2cm），整个乐音体系中约有88个音。其中小字一组的a音[机械波](https://baike.baidu.com/item/机械波/2734777)为440Hz,[波长](https://baike.baidu.com/item/波长/829184)78cm,称为标准音.现代钢琴是音域最宽的乐器，能奏出88个音高不同的乐音。

timbres  （音色）

### （reverberation）混响

[混响](https://baike.baidu.com/item/%E6%B7%B7%E5%93%8D/480460?fr=aladdin)

声波在室内传播时，要被墙壁、天花板、地板等障碍物反射，每反射一次都要被障碍物吸收一些。这样，当声源停止发声后，声波在室内要经过多次反射和吸收，最后才消失，我们就感觉到声源停止发声后还有若干个声波混合持续一段时间（室内声源停止发声后仍然存在的声延续现象）。这种现象叫做混响，这段时间叫做混响时间。

对讲演厅来说，[混响时间](https://baike.baidu.com/item/混响时间)不能太长．我们平时讲话，每秒钟大约发出2～3个单字，假定发出两个单字“物理”，设想混响时间是3秒，那么，在发出“物”字的声音之后，虽然声强逐渐减弱，但还要持续一段时间(3秒)，在发出“理”字的声音的时刻，“物”字的声强还相当大。因而两个单字的声音混在一起，什么也听不清楚了。但是，混响时间也不能太短，太短则响度不够，也听不清楚。因此需要选择一个最佳混响时间．北京科学会堂有一个学术报告厅，混响时间为1秒。

### Feature pyramid network  

FPN特征金字塔结构，为解决object detection任务中过小的物体检测困难。

<img src="C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105105400538.png" alt="image-20211105105400538" style="zoom: 50%;" />

识别小物体，随着卷积加深，会损失一定分辨率，很有可能小物体部分的分辨率会在高层cnn完全丢失掉，所以只从高级语义特征预测object势必导致预测丢失

#### 传统图像金字塔（Featurized Image Pyramid)

既然缩小图片尺寸导致object丢失，那么我在每一个尺度的图像上都进行预测来避免丢失。缺点，耗时。

![image-20211105105720339](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105105720339.png)

#### Naive CNN

CNN有层次的学习图片的特征，深度越深，特征的语义越高级。但是由于池化以及降采样，浅层网络可以保留更多的分辨率，但是语义特征较低，深层网络具有强的语义信息，但是对于小物体，其分辨率随着降采样根本难以在高级网络中保留下来。

![](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105105857712.png)

#### 金字塔形特征层级（Pyramidal feature hierarchy）

SSD。但是其实际上没有完全使用底层信息。

![image-20211105110215885](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105110215885.png)

#### 特征金字塔网络

想较于SSD，使用了特征跳连的方式联系千层特征以及深层特征。这和U-Net结构很像。

<img src="C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105110417338.png" alt="image-20211105110417338" style="zoom:67%;" />

### 2020_NIPS_Audeo: Audio Generation for a Silent Performance Video

#### Motivation

#### Methods

- retrieve the Pseudo Ground Truth (GT) Midi from the audio with the Onset and Frames framework. The pseudo GT Midi is $M \in R^{K \times T}$ .$K$是pitchs个数（即钢琴键个数，88）$T$代表帧个数。

![image-20211102121020803](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211102121020803.png)

##### Video2Roll Net

- 这一阶段可以看做是解决multi-label image classification problem  
- $P(\hat{M}_{:,t})=P(M_{:,t}|X_{t-2:t+2})$ .输入是t时刻前后共计五帧，输出是中间时刻帧pressed keys的预测概率。
- 连续帧之间的动作对于本任务十分重要（the use of consecutive frames is critical to detect changes in the pressed keys.  ）
- ？？？ estimate all pressed keys at each frame **harder** prediction of onsets events only（which and when a key  is being pressed），我没看出来这两种预测方式的差别。
- Consider takes into consideration the natural phenomena appearing in this task:   
  - the visual cues of the sustained keys are relatively small compared to other objects in the image such as hands and fingers;  
  - at each frame, the pressed keys may correlate due to the concept of musical harmony so **some combinations have a higher chance to appear at the same time than others**;   
  - the spatial dependencies are significant to detect the sustained keys but the typical CNN is designed to be invariant to spatial **positioning**.   

- 网络结构借鉴了FPN金字塔结构。不同尺度的特征进行融合。



![image-20211102121039301](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211102121039301.png)



![image-20211102121051262](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211102121051262.png)

##### Roll2Midi Net

- Video2Roll存在一些问题，

  - hand occlusion，手遮挡
  - Roll predictions not have temporal connection，没有时域上的联系（这里指的应该是前文说过的相同的组合在同一个音乐中是会重复出现的，这种相同的组合分布在时域上，没有有效的关联起来）
  - 由于是在每个时刻预测那个键被摁下去了，但是实际上钢琴一个键摁下去很长时间，这个键产生的声音会逐渐衰减到0，但是我们依旧对每个摁下去的键都做相同的处理，这导致我们预测出来的Midi和从audio产生的pesudo Midi存在差别。

- 为了解决上述问题，采用GAN的方式来生成Midi（感觉GAN就是一种抽象的约束）

  $ Min_GMax_DE_{M\_m}[logD(M)]+E_{\hat{M}\_\hat{m}}[log(1-D(G(\hat{M})))]  $ 

  

![image-20211105112642884](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105112642884.png)

![image-20211105130041296](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105130041296.png)

![image-20211105130053188](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211105130053188.png)

##### Midi Synth

主要用到了Score-to-audio music generation with multi-band convolutional residual network  提到的算法进行Midi / Roll到频谱的重建。

![img](https://pic3.zhimg.com/v2-6e7dba930f44c3440352bc4ed470754e_b.jpg)

#### Dataset

- 数据集采用了YouTube上的钢琴教学视频，都是这种俯拍图。with a fully visible keyboard
- 输入到网络中的视觉信息是经过裁剪的，只有钢琴键区域的图像被送入到网络。crop all videos and keep the full keyboard only and remove all frames that do not contribute to the piano performance  

**Pseudo Midi Evaluation Set:**

- use to evaluate Video2Roll Net and Roll2Midi Net
- 测试集的大小是24个video一共115minutes（蛮小的），测试集大小是3的video一共12.5min。但是fps=25，所以他得到172404 training images和18788 test images 

**Audio Evaluation Set**

- 数据集大小，一共有52个 videos，共计297 minutes。

![image-20211103195115354](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211103195115354.png)

#### Evaluation

- 对于**Midi Evaluation** ，采用reporting the precision, the recall, the accuracy, and the F1 score on the frame-level  
- 对于**Audio Evaluation**，使用一个APP SoundHound来做判断，判断我们生成的音乐是哪种类别。

![image-20211103214516046](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211103214516046.png)

### SIGHT TO SOUND: AN END-TO-END APPROACH FOR VISUAL PIANO TRANSCRIPTION  

#### Motivation

- Automatic music transcription has primarily focused on transcribing audio to a **symbolic music representation (e.g. MIDI or sheet music).**   传统的music转录是从audio映射生成一个符号音乐表示，比如MIDI或者乐谱。但是这种方法会受到polyphonic instruments and background noise 多音乐器以及背景噪音的影响。
- 所以本文直接transcribing piano music from visual

#### Introduction

- **AMT**（Automatic music transcription）从audio中得到MIDI编码
- **VMT** （Visual music transcription）从视觉信息转录的到MIDI。
- 本文使用AMT的方法来制造 pesudo label
- Using video removes the ambiguities that arise from relying on audio alone when multiple notes sound simultaneously.  使用VMT较于AMT优势在于VMT可以消除多个钢琴键同时摁下，产生多个audio，mix audio会导致重叠的问题！！！
- Visual infomation。localization of hands and keys可以帮助定位到那个键被摁下。motion between frames帮助判断钢琴键被摁下的开始时刻。

#### Network

- 主体backbone是ResNet-18，只不过在中间有两处做了变化。一处是3D卷积，聚合多帧信息，一处是引入position embedding。
- CNN有一定的探索位置信息的能力，但是预测钢琴位置对于位置信息要求比较严格，所以需要更多的位置信息，故设计了一个slope vector $x_{key}=[0,1]^{88}$ ，长度88正好代表88个钢琴键。
- loss。使用了cross-entropy loss。对88个键每个键做是否为摁下起始点的分类预测。

![image-20211104185611564](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211104185611564.png)

#### Dataset

**PianoYT** 

- over 20 hours。209/19 = train/test
- 使用Onsets and Frames framework [15]. 获得pseudo MIDI ground-truth

**MIDI test set**

- 自己录的一段视频，可以看做一个benckmark。MIDI是用phone camera得到的，更准确。

**Two Hands Hanon test set**



### 2020_ECCV_Foley Music: Learning to Generation Music from Videos

#### Motivation

- Two key intermediate representations(中间表示) for a successful video to music generation
  - body keypoints from videos
  - Midi event from audio recordings
- the MIDI representations are fully interpretable and transparent, thus enabling us to perform **music editing** flexibly.   

![image-20211108104258096](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211108104258096.png)

#### Introduction

- McGurk effect  

  “麦格克效应”(McGurk effect)，是一种感性的认知现象，表现出在语音感知过程中[听觉](https://baike.baidu.com/item/听觉/1355102)和[视觉](https://baike.baidu.com/item/视觉/5228)之间的相互作用，有时人类的听觉会过多的受到视觉的影响，从而产生[误听](https://baike.baidu.com/item/误听/9760580)的现象。当视觉看到的一种声音与耳朵听到的另一种声音不匹配时，会让人们神秘的察觉到第三种声音。

  在给自愿者放映的一部影片中，一个音节“ga”在配音时发作了“ba”，而自愿者称听到的音节是却是“da”。这样一来，视听信息联手创造出了第三种全新的声音，这个过程被叫做“麦格克效应”（McGurk effect）。

  视觉信息的不一致可以改变对于口语发音的感知。

- Graph-Transformer module
  - GCN encoder takes input the coordinates of detected keypoints and applies a spatial-temporal graph conv strategy to produce latent feature vectors over time
  - Transformer decoder capture the long-term relationships between human body motion and Midi events using self-attention mechanism

#### Approach

##### Visual and audio Representation

**Visual Representation**

(Human Pose features) the explicit movement of the human body parts and hand fingers   

使用open-source OpenPose toolbox提取 2D coordinates of human body joints

OpenPose hand API提取the coordinates of hand keypoints

我们得到25个human body keypoints，21个hand keypoints

**Audio Representation**

选取合适的audio表示对于music generation十分重要

directly generate raw waveform using RNN

predict sound spectrograms using GAN

上面两种表示都不行，因为music is highly compositional and contains many structured events 。网络直接学习到音乐的规律，法则是很困难的。

使用Midi，包括note-on,note-off,note pitch 等信息

##### Body Motions to MIDI Predictions

Graph-Tansformer  model :  first adopt a spatial-temporal graph convolutional network on body keypointcoordinates over time to capture body motions and then feed the encoded pose features to a music transformer decoder to generate a sequence of the MIDIevents.  

**Visual Encoder**

Graph CNN

- represent human skeleton sequence as an undirected spatial-temporal graph $G=(V,E)$ .顶点就是关键点，edge则是keypoints的自然连接构成。
- first perform a spatial GCN to encode pose features at each frame (即每个时刻所有顶点边映射成一个pose feature)
- a spatial temporl conv to aggregate the temporal cues.

$P=AXW_SW_T$

$X\in R^{V\times T\times C_n}$  ,V代表nums of keypoints，$C_n$代表feature dimension。每一帧，对每个顶点的坐标进行编码。

$A \in R^{V \times V}$  ,graph的邻接矩阵。

$W_S$ -- wight of spatial graph conv; $W_T$ -- weight of temporal graph conv

$P\in R^{T_v \times C_v}$ 代表pose feature

**MIDI Decoder**

预测Midi来生成音乐本质是序列生成问题。

Transformer --- an encoder-decoder based autoregressive(自回归) generative model(机器翻译)

Transformer输入 ，$P\in R^{T_v\times C_v}$ , 输出 , $M\in R^{T_m\times L}$ , $T_m$ 代表MIDI events numbers，$L$代表一个event的长度（一般是16进制码） 

- positional representation, which allow attentions explicitly know the distance between two tokens; jointly learn an ordered relative position embedding R   

  $Relative Attention (Q,K,V)=Softmax(\frac{QK^t+R}{\sqrt{D_k}})V$ 

- output of Masked Self-Attention Layer is $M\in R^{T_m\times C_m}$ ; So Multi-head Cross Attention Layer input is M and pose feature $P\in R^{T_v \times C_v}$ 

![image-20211109151422880](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211109151422880.png)

##### Training and Inference

The training objective is to minimize the cross-entropy loss given a source target sequence of MIDI events.  

### 2021_BEiT

#### Motivation

希望图像预训练也能够使用类似Bert的预训练方式进行训练。NLP中，把句子中的几个单词Mask掉然后使用Transformer结构预测出mask部分的单词。通过完成这种“完形填空”的上游任务，模型完成ssl（self-supervised learning）预训练。本文希望在CV领域使用相同的Mask-预测上游任务完成模型ssl预训练

#### Introduction

- a **self-supervised** vision representation model BEIT --- Bidirectional Encoder representation from Image Transformers  

- a **masked image modeling task** to pretrain vision Transformers ---- masked image modeling (MIM)  

- vision Transformers require more training data than convolutional neural networks.   

  solve the data-hungry issue, self-supervised pre-training is a promising solution to leverage large-scale image data.  

  visual Transformer比CNN需要更多的训练data，所以我们希望能够通过自监督的方式得到预训练好的模型

- 完成任务的一种选择是把复原图像当做一个regression（回归）任务，但是有waste modeling capability on pre-training
  short-range dependencies and high-frequency details problems。模型得不到短程依赖以及高精度细节，也就是说每个像素点进行回归预测，没有使用到图像具有的局部性，空间联系等固有特性。（（（这一点有疑问的其实，使用Transformer做image预训练应该避免这点，即避免模型过于关注局部信息，我们更想要的是high-level abstraction）））

- two views of images -- image patches and visual tokens which serve as input and output. tokens是通过[VAE](https://arxiv.org/abs/2102.12092)得到的

- 在没有任何人工标注的前提下，我们的模型学习到了语义区域



![image-20211114184232755](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211114184232755.png)

#### Method

##### Image Representation

**Image Patch**

raw image $x\in R^{H\times W\times C}$ 

$N=HW/P^2$ 个patches $x^p\in R^{N\times(P^2C)}$ 

In experiments $224\times224$ === $14\times14$ patches, 每个patch大小$16\times 16$ 

每个patch会flatten成一个256维度的vector

**Visual Token**

- 和NLP很像，我们把image转化成 a sequences of discrete tokens，我们希望模型来预测离散的tokens而不是预测raw image（是因为连续的图片，像素级别预测做不到嘛）

通过VAE，iamge$x\in R^{H\times W\times C}$ === tokens $z=[z_1,...,z_N]\in V^{h\times w}$ 

dVAE 首先使用Tokenizer Block 把image pixels转化为 token $z$ ((这里Tokenizer Block充当了字典的作用，得到的token称为 visual codebook))。 之后通过离散的tokens $z$ 来重建图像。

这里vocabulary size设置为了 $|V| = 8192$

##### Backbone Network : Image Transformer

和**ViT**一致

##### Pre-Training BEiT : Masked Image Modeling

- 我们随机mask掉40%的patches，mask position $M\in \{1,...,N\}^{0.4N}$ 

  被mask掉的patches被替换成一个可学习的embedding $e_{[M]}\in R^D$ 

  所以最终送到Transformer里头的input是 $x^M=\{x_i^p:i\notin M \}\cup\{e_{[M]}:i\in R^D\}$ 

  类似ViT，只使用Transformer编码器部分，并认为编码器最后一层能够起到解码的效果

  对于每个mask patch，我们使用softmax来预测重建。 

- 除了随机mask，我们还采取了Blocking Masking的方式

  

![image-20211114203103719](C:\Users\zynlo\AppData\Roaming\Typora\typora-user-images\image-20211114203103719.png)

##### From the Perspective of Variational Autoencoder

